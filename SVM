#第一章导论
学习：自动找出输入（测量）输出之间的依赖结构
为什么自动？因为输入太复杂。
通过已知数据建立一套规则，预测未来值。
假设一个p(x,y)的联合概率密度存在。先用边缘概率产生x,再用条件概率产生y。
参数模型：输入输出之间的函数类型被假定。
期望损失，即函数f的风险。风险用来描述f的质量。
我们希望找到一个函数在不知道联合概率密度p的情况下具有最优的响应性能。
设置一种学习方法和一个收敛速度，一定存在一个分布P无法再规定时间内找到f。
对P进行一定的限制，可保证学习率。
什么是经验风险最小化。
存在问题：我们的知识通常不足以对f做出合理假设，计算量可能过大
对于二值分类问题，用hinge loss转化为凸优化问题。
只要hinge loss学习得很好，那么相应的classification loss也会接近最优
用凸的代理损失去代替非凸损失。
svm方法对P对H进行假设。
线性不可分的情况，必须加入松弛变量，在更高纬度求解。实践上使用拉格朗日方法，计算相应的对偶规划问题。
为什么能绕过feature mapping（加速计算）。不用计算从低位空间到高位空间的映射本身，直接计算核函数。

#A3 度量与积分理论
X是一个集合，A是它的子集的集合。若X属于A，且A集合上满足补集与并集的封闭性，则A是在X上的δ代数
# 第二章 损失函数及其风险
